# -*- coding: utf-8 -*-
"""final_Nik_Baldis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15mywPdNDvxf7Y5amkHlpfFZ5xXScKHMv

# Final Project
---

- Your Name Here: Niklas Baldis
- WFU Email Address: baldnj23@wfu.edu
- Submission Date: 8/20/23

#Import Libraries and Data and Clean Up the Data
"""

import warnings
warnings.filterwarnings('ignore')
# --------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# --------------------------------
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# --------------------------------
# Suppress scientific notation in the notebook for pandas DataFrame
pd.set_option('display.float_format', '{:.4f}'.format)

boston = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Analytics Software Tech- SS/Projects/Project 6 Final/Data/boston-1.csv')
boston.head()

boston.info()

"""#Finding and Dropping Nulls"""

boston.isnull().sum()

boston.dropna(inplace=True)
boston.isnull().sum()

"""#Numeric Data Exploratory Analysis

Filtering Data Based on Z-Scores to remove outliers
"""

from scipy.stats import zscore
# Calculate Z-scores
z_scores = zscore(boston['av_total'])
# Get boolean array indicating the presence of outliers
# Here, we consider data points with Z-scores > 2 or < -2 as outliers
outliers = np.where((z_scores > 2) | (z_scores < -2))
# Filter out the outliers
boston = boston[~(z_scores > 2) | (z_scores < -2)]

numeric_columns = boston.select_dtypes(include=[np.number]).columns
numeric_columns

"""Histograms of Numerical Features to Observe the Distribution of the Data"""

numeric_features = ['land_sf', 'yr_built', 'yr_remod', 'living_area',
       'num_floors', 'r_total_rms', 'r_bdrms', 'r_full_bth', 'r_half_bth',
       'r_kitch', 'r_fplace','population', 'pop_density',
       'median_income','av_total']
for feature in numeric_features:
  plt.figure(figsize=(10, 3))
  sns.histplot(boston,x=feature, bins=30, kde=True,)
  plt.title(f'Histogram of {feature}')
  plt.xlabel(feature)
  plt.show()

"""Some graphs that stand out from this analysis are the land square footage, the year built, and the living area.  The majority of land square footage seems to be between 5000 and 10000 square feet.  The majority of homes built are from the early to mid 20th century, and the majority of living area seems to be about 1400 square feet.

Boxplots of Numerical Features to Identify Outliers
"""

for feature in numeric_features:
  plt.figure(figsize=(10, 3))
  sns.boxenplot(boston,x=feature)
  plt.title(f'Boxplot of {feature}')
  plt.xlabel(feature)
  plt.show()

"""There are definitely a few outliers noticeable, especially in land square footage, living area, and total number of rooms.

#Statistical Analysis of Numerical Features
"""

boston[numeric_features].describe().transpose()

"""Scatter Plots of Total Assessed Value and Numerical Features"""

for feature in numeric_features:
  if feature != 'av_total':
    plt.figure(figsize=(10, 3))
    sns.scatterplot(boston,x=feature, y='av_total')
    plt.title(f'Scatter Plot of Assessed Value by {feature} ')
    plt.xlabel(feature)
    plt.ylabel('Assessed Value')
    plt.show()

"""Correlations - The linear relationship between Each Variable and Total Assessed Value"""

# Compute the correlation of each variable with the target
correlations = boston[numeric_features].apply(lambda x: x.corr(boston['av_total']))
boston_correlations = correlations.to_frame(name='correlation')
boston_correlations = boston_correlations.sort_values('correlation', ascending=False)
boston_correlations

"""Heatmap- Easier Visualization of Numeric Correlations"""

plt.figure(figsize=(8, 6))
sns.heatmap(boston_correlations,
            annot=True,
            cmap='coolwarm',
            vmin=-1,
            vmax=1,
            cbar_kws={'label': 'Correlation Coefficient'})
plt.title('Correlation of Numeric Features with Assessed Value')
plt.show()

"""It is interesting that median income has a strong positive correlation with assessed value, maybe it is because the owners can afford more living area, which has an even stronger correlation with assessed value.  It is also interesting that, as the year bulit increases, the assessed value decreases.

#Categorical Data Exploratory Analysis
"""

categorical_columns = boston.select_dtypes(include=['object']).columns
categorical_columns

categorical_features = ['zipcode','own_occ', 'structure_class', 'r_bldg_styl', 'r_roof_typ', 'r_ext_fin',
       'r_bth_style', 'r_kitch_style', 'r_heat_typ', 'r_ac', 'r_ext_cnd',
       'r_ovrall_cnd', 'r_int_cnd', 'r_int_fin', 'r_view', 'city_state']

for cat in categorical_features:
  print(cat)
  print(boston[cat].value_counts())
  sns.countplot(data=boston,x=cat, color='lightblue')
  plt.show()

"""It makes sense that a lot of houses are colonial style and dont have ac, as Boston is an old city.  The conditions of the houses and its features are relatively good, which means that the city does well to maintain its buildings."""

for category in categorical_features:
  plt.figure(figsize=(10, 3))
  sns.boxplot(boston,x=category, y='av_total')
  plt.title(f'Boxplot of Assessed Value by {category} ')
  plt.xlabel(category)
  plt.ylabel('Assessed Value')
  plt.show()

"""One thing that really stands out from the box plots is that Jamaica Plain properties may be a good place to invest, as they have a noticeably higher assessed value than the other cities.  It is also expected that the conditions of the house and its features seem to have a pretty strong relationship.

#Scatter Plots of Total Assessed Value and Numerical Features
"""

for feature in categorical_features:
  if feature != 'av_total':
    plt.figure(figsize=(10, 3))
    sns.scatterplot(boston,x=feature, y='av_total')
    plt.title(f'Scatter Plot of Assessed Value by {feature} ')
    plt.xlabel(feature)
    plt.ylabel('Assessed Value')
    plt.show()

"""#Correlations - Correlation of All Variables to Assessed Value Total"""

correlation_matrix_boston = boston.corr()
plt.figure(figsize=(15, 15))
sns.heatmap(correlation_matrix_boston, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Heatmap of Correlation Coefficients')
plt.show()

"""It makes sense that living area, number of floors, total rooms, and total bedrooms have a strong correlation, as having more living area would lead to bigger houses.

#Specific Client Questions
"""

boston.columns

boston.info()

"""#Owner-occupied homes have a higher assessed value"""

oo_home_yes_filter= boston['own_occ']=='Y'
oo_home_yes_filter

oo_home_yes= boston[oo_home_yes_filter]
oo_home_yes

oo_home_no_filter= boston['own_occ']=='N'
oo_home_no_filter

oo_home_no= boston[oo_home_no_filter]
oo_home_no

av_total_home_oo_yes = oo_home_yes['av_total'].mean()
print('Average Assessed Value of Owner Occupied Home:', '$',av_total_home_oo_yes)
av_total_home_oo_no = oo_home_no['av_total'].mean()
print('Average Assessed Value of Home Not Owner Occupied:', '$',av_total_home_oo_no)

"""#Homes built in the 1990s have higher home values"""

home_1990s = boston.query('1990 <= yr_built <= 2000')
home_1990s

home_less_1990s = boston.query('yr_built < 1990')
home_less_1990s

home_great_2000s = boston.query('yr_built > 2000')
home_great_2000s

home_not_1990s= pd.concat([home_less_1990s, home_great_2000s], ignore_index=True)
home_not_1990s

av_total_home_1990s = home_1990s['av_total'].mean()
print('Average Assessed Value of Home Built in the 1990s:', '$',av_total_home_1990s)
av_total_home_not_1990s = home_not_1990s['av_total'].mean()
print('Average Assessed Value of Home Not Built in the 1990s:', '$',av_total_home_not_1990s)

"""#Homes that have been recently remodeled have higher home values"""

home_remod_recent = boston.query('yr_remod >= 2000')
home_remod_recent

home_remod_not_recent = boston.query('yr_remod < 2000')
home_remod_not_recent

av_total_home_remod_recent = home_remod_recent['av_total'].mean()
print('Average Assessed Value of Home Recently Remodeled:', '$',av_total_home_remod_recent)
av_total_home_remod_not_recent = home_remod_not_recent['av_total'].mean()
print('Average Assessed Value of Home Not Recently Remodeled:', '$',av_total_home_remod_not_recent)

sns.pairplot(boston[numeric_features].sample(1000))
plt.title('Pairs Plot')
plt.show()

"""#Model 1 Analysis

#Merging Predictions to Boston Dataframe
"""

pred1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Analytics Software Tech- SS/Projects/Project 6 Final/Data/model1_predictions-1.csv')
pred1.head()

boston_prep1 = boston.merge(pred1, on='pid')
boston_prep1.info()

"""#Evaluating Model 1

Getting Dummies
"""

boston_mod1_dummies = pd.get_dummies(boston_prep1, columns = ['zipcode','own_occ', 'structure_class', 'r_bldg_styl', 'r_roof_typ', 'r_ext_fin',
       'r_bth_style', 'r_kitch_style', 'r_heat_typ', 'r_ac', 'r_ext_cnd',
       'r_ovrall_cnd', 'r_int_cnd', 'r_int_fin', 'r_view', 'city_state'])

boston_mod1_dummies.head()

boston_mod1_dummies.columns

boston_mod1_dummies.drop(columns=['pid','zip'],inplace=True)
boston_mod1_dummies.head()

X = boston_mod1_dummies.drop(columns=['av_total'])
y = boston_mod1_dummies['av_total']
linear_reg = LinearRegression()
linear_reg.fit(X, y)
linear_reg

features_df = pd.DataFrame({'Feature': linear_reg.feature_names_in_, 'Coefficient': linear_reg.coef_})
features_df

intercept_df = pd.DataFrame({'Feature': ['const'], 'Coefficient': [linear_reg.intercept_]})
intercept_df

model_table = pd.concat([intercept_df, features_df], axis=0)
model_table

import statsmodels.api as sm
# Adding a constant to the model (intercept)
X_train_with_const = sm.add_constant(X)
# Fit the regression model using statsmodels
model_sm = sm.OLS(y, X_train_with_const).fit()
# Extracting p-values
p_values = model_sm.pvalues
p_values_df = pd.DataFrame(p_values, columns=['P-Value']).round(3).reset_index()
p_values_df

model_table_df = model_table.merge(p_values_df, left_on='Feature', right_on='index').drop(columns=['index'])
model_table_df

"""Finding what Features do not Affect the Assessed Value of a House"""

model_table_df[model_table_df['P-Value'] > 0.05 ]

# calculate stats
reg_prediction_mod1 = boston_prep1['pred']
r2_mod1 = r2_score(y, reg_prediction_mod1)
mse_mod1 = mean_squared_error(y, reg_prediction_mod1)
mae_mod1 = mean_absolute_error(y, reg_prediction_mod1)

print("-- Linear Regression Stats for Assessed Value Total -- ")
print(f'R-Square: {r2_mod1:.3f}')
print(f" - RSQUARE: approximately {r2_mod1:.1%} of the variability in the total assessed value can be explained by model 1.")

print(f'Root Mean Squared Error: {mse_mod1**0.5:,.2f}')
print(f" - RMSE: on average, our predictions are approximately ${mse_mod1**0.5:,.2f} away from the actual assessed value")
print(f'Mean Absolute Error: {mae_mod1:,.2f}')
print(f" - MAE:  on average, the predictions made by the model are off by +/- ${mae_mod1:,.2f} from the actual")

"""#Creating Column for Residual Error"""

boston_prep1['residual_error'] = boston_prep1['av_total'] - boston_prep1['pred']
boston_prep1.head()

"""#Plotting Residuals"""

import scipy.stats as stats

# Set up a 2x2 grid for plotting
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))

# Plot histogram of residuals
sns.histplot(boston_prep1['residual_error'], kde=True, ax=axes[0, 0])
axes[0, 0].set_title('Histogram of Residuals')

# Scatter plot of predicted values vs. residuals
sns.scatterplot(data=boston_prep1, x='pred', y='residual_error', ax=axes[0, 1])
axes[0, 1].axhline(y=0, color='r', linestyle='--')
axes[0, 1].set_title('Residuals vs. Predicted Values')

# Plot residuals over order (using index as a proxy for order)
sns.scatterplot(data=boston_prep1, x='yr_built', y='residual_error', ax=axes[1, 0])
axes[1, 0].axhline(y=0, color='r', linestyle='--')
axes[1, 0].set_title('Residuals Over Order')


# Q-Q plot of residuals
stats.probplot(boston_prep1['residual_error'], dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Q-Q Plot of Residuals')

# Adjust layout
plt.tight_layout()
plt.show()

"""By filtering the data to remove outliers, the Q-Q plot shows a very strong linear relationship, but the tail is very heavy on the left side.

#Looking at Overestimates, Underestimates, and Best Predictions

Overestimate
"""

boston_prep1.nsmallest(10,'residual_error')

"""Underestimate"""

boston_prep1.nlargest(10,'residual_error')

"""Best Estimates"""

boston_prep1['abs_residual'] = boston_prep1['residual_error'].abs().round(3)
boston_prep1.nsmallest(10,'abs_residual')

"""#Model 2 Analysis

#Merging Prediction to Dataframe
"""

pred2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Analytics Software Tech- SS/Projects/Project 6 Final/Data/model2_predictions-1.csv')
pred2.head()

boston_prep2 = boston.merge(pred2, on='pid')
boston_prep2.info()

"""#Evaluating Model 2

Getting Dummies
"""

boston_mod2_dummies = pd.get_dummies(boston_prep1, columns = ['zipcode','own_occ', 'structure_class', 'r_bldg_styl', 'r_roof_typ', 'r_ext_fin',
       'r_bth_style', 'r_kitch_style', 'r_heat_typ', 'r_ac', 'r_ext_cnd',
       'r_ovrall_cnd', 'r_int_cnd', 'r_int_fin', 'r_view', 'city_state'])

boston_mod2_dummies.head()

boston_mod2_dummies.columns

boston_mod2_dummies.drop(columns=['pid','zip'],inplace=True)
boston_mod2_dummies.head()

X = boston_mod2_dummies.drop(columns=['av_total'])
y = boston_mod2_dummies['av_total']
linear_reg = LinearRegression()
linear_reg.fit(X, y)
linear_reg

features_df = pd.DataFrame({'Feature': linear_reg.feature_names_in_, 'Coefficient': linear_reg.coef_})
features_df

intercept_df = pd.DataFrame({'Feature': ['const'], 'Coefficient': [linear_reg.intercept_]})
intercept_df

model_table = pd.concat([intercept_df, features_df], axis=0)
model_table

import statsmodels.api as sm
# Adding a constant to the model (intercept)
X_train_with_const = sm.add_constant(X)
# Fit the regression model using statsmodels
model_sm = sm.OLS(y, X_train_with_const).fit()
# Extracting p-values
p_values = model_sm.pvalues
p_values_df = pd.DataFrame(p_values, columns=['P-Value']).round(3).reset_index()
p_values_df

model_table_df = model_table.merge(p_values_df, left_on='Feature', right_on='index').drop(columns=['index'])
model_table_df

"""Finding what Features do not Affect the Assessed Value of a House"""

model_table_df[model_table_df['P-Value'] > 0.05 ]

# calculate stats
reg_prediction_mod2 = boston_prep2['pred']
r2_mod2 = r2_score(y, reg_prediction_mod2)
mse_mod2 = mean_squared_error(y, reg_prediction_mod2)
mae_mod2 = mean_absolute_error(y, reg_prediction_mod2)

print("-- Linear Regression Stats for Assess Value Total -- ")
print(f'R-Square: {r2_mod2:.3f}')
print(f" - RSQUARE: approximately {r2_mod2:.1%} of the variability in the total assessed value can be explained by model 2.")

print(f'Root Mean Squared Error: {mse_mod2**0.5:,.2f}')
print(f" - RMSE: on average, our predictions are approximately ${mse_mod1**0.5:,.2f} away from the actual assessed value")
print(f'Mean Absolute Error: {mae_mod2:,.2f}')
print(f" - MAE:  on average, the predictions made by the model are off by +/- ${mae_mod2:,.2f} from the actual")

"""Model 2's R-Square value is much higher than Model 1's indicating that it is much more accurate at predicting the assessed value of a house than model 1.

#Creating Column for Residual Error
"""

boston_prep2['residual_error'] = boston_prep2['av_total'] - boston_prep2['pred']
boston_prep2.head()

"""#Plotting Residuals"""

import scipy.stats as stats

# Set up a 2x2 grid for plotting
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))

# Plot histogram of residuals
sns.histplot(boston_prep2['residual_error'], kde=True, ax=axes[0, 0])
axes[0, 0].set_title('Histogram of Residuals')

# Scatter plot of predicted values vs. residuals
sns.scatterplot(data=boston_prep2, x='pred', y='residual_error', ax=axes[0, 1])
axes[0, 1].axhline(y=0, color='r', linestyle='--')
axes[0, 1].set_title('Residuals vs. Predicted Values')

# Plot residuals over order (using index as a proxy for order)
sns.scatterplot(data=boston_prep2, x='yr_built', y='residual_error', ax=axes[1, 0])
axes[1, 0].axhline(y=0, color='r', linestyle='--')
axes[1, 0].set_title('Residuals Over Order')


# Q-Q plot of residuals
stats.probplot(boston_prep2['residual_error'], dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Q-Q Plot of Residuals')

# Adjust layout
plt.tight_layout()
plt.show()

"""Although the Q-Q plot for Model 2 shows that the residuals are not as normally distributed as Model 1's residuals, there is still a strong normallity to the data

#Looking at Overestimates, Underestimates, and Best Predictions

Overestimate
"""

boston_prep2.nsmallest(10,'residual_error')

"""Underestimate"""

boston_prep2.nlargest(10,'residual_error')

"""Best Estimates"""

boston_prep2['abs_residual'] = boston_prep2['residual_error'].abs().round(3)
boston_prep2.nsmallest(10,'abs_residual')

#Convert to HTML
#%%shell
#jupyter nbconvert --to html "/content/drive/MyDrive/Colab Notebooks/Analytics Software Tech- SS/Projects/Project 6 Final/final_Nik_Baldis.ipynb"