# -*- coding: utf-8 -*-
"""Baldis_Nik_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cCb2lGbxPG_xZ1ivfe0Dgo1T98rcJ7WK
"""

# install face_recognition package

import subprocess
import sys

# -- uncomment --

subprocess.check_call([sys.executable, "-m", "pip", "install", "face_recognition"])
#!pip install face_recognition

# import libraries
import requests
import zipfile
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# image stuff
import face_recognition
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageOps, ImageEnhance
from google.colab.patches import cv2_imshow

from google.colab import drive
drive.mount('/content/drive')

"""end step 1

# Automation
---

1. get a list of all files
2. generate encodings of faces
3. loop through matches
    - catch similarity
"""

!pip install gdown

!gdown --id 1BC58HuMp6V4pLGF1fpAFgsbSKVp4hlyQ

!unzip -o "msba_students_2024.zip"

files=os.listdir('/content/content/SAM/2024_students')
files.remove('.ipynb_checkpoints')
print(files)

def student_face_load_source():
  student_face_dictionary ={}

  for file in files:
    try:
      img_file = '/content/content/SAM/2024_students/{}'.format(file)
      print(img_file)
      _t= face_recognition.load_image_file(img_file)
      _x= face_recognition.face_encodings(_t)[0]
      student_face_dictionary[img_file]= _x

    except:
      _x = 0
      student_face_dictionary[img_file] = _x
      print('No face found in image')

  return student_face_dictionary
student_face_encodings= student_face_load_source()

import pickle
with open('encodings.pickle','wb') as f:
  pickle.dump(student_face_encodings,f)

with open('encodings.pickle', 'rb') as f:
  student_face_encodings= pickle.load(f)

student_face_encodings

"""#AWS Configuration"""

!pip install boto3

import boto3
import json
import boto3
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from io import BytesIO
from IPython.display import display

!pip install awscli

! aws configure

from botocore.exceptions import NoCredentialsError
import os

aws_bucket = 'baldis-msba-headshots'

s3 = boto3.client('s3')
Student_Faces_In_Bucket = s3.list_objects_v2(Bucket = aws_bucket)
#check that bucket isn't empty
if 'Contents' in Student_Faces_In_Bucket:
  for item in Student_Faces_In_Bucket['Contents']:
    print(f"    {item['Key']}")

"""#Use Case 1: AWS"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

match_file = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/FinalMatchSourceTarget.xlsx")
match_file.head()

#replace jpeg with jpg
match_file['source_image'] = match_file['source_image'].str.replace('.jpeg','.jpg')
match_file['target_image'] = match_file['target_image'].str.replace('.jpeg','.jpg')
match_file = match_file[match_file['source_image'] != '_aged.jpg']
match_file = match_file[match_file['target_image'] != '_aged.jpg']

match_file['source_image'] = match_file['source_image'].str.strip()
match_file['target_image'] = match_file['target_image'].str.strip()

match_file.head()

# Compare Faces Now and Aged
def compare_faces(bucket, sourceFile, targetFile):
  client= boto3.client('rekognition', region_name= 'us-east-2')
  similarity= 0 #default value in case no match is found

  try:
    response = client.compare_faces(SimilarityThreshold= 60,
                                    SourceImage={'S3Object': {'Bucket': bucket, 'Name': sourceFile}},
                                    TargetImage={'S3Object': {'Bucket': bucket, 'Name': targetFile}})
    #Check if any face matches are found

    if response['FaceMatches']:
      faceMatch= response['FaceMatches'][0] #get the first match
      similarity= faceMatch["Similarity"]
      print(similarity)

  except Exception as e:
    print(f"An error occured: {e}")

  return similarity

match_file['match_similarity']= match_file.apply(lambda row: compare_faces(aws_bucket, row['source_image'], row['target_image']), axis=1)

match_file.head()

import numpy as np
match_file['predicted_match'] = np.where(match_file['match_similarity'] > 60, 'match','no match')
match_file.head()

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

# Calculate confusion matrix
matrix = confusion_matrix(match_file['expected_match'],
                          match_file['predicted_match'],
                          labels=["match", "no match"])

print(matrix)

# Calculate confusion matrix
y_true = match_file['expected_match']
y_pred = match_file['predicted_match']
labels = ["match", "no match"]
matrix = confusion_matrix(y_true, y_pred, labels=labels)

# Plotting using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(matrix, annot=True, fmt="d", xticklabels=labels, yticklabels=labels, cmap="Blues")
plt.title("Confusion Matrix")
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()



# Calculate Accuracy, Precision and Recall
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, pos_label="match")
recall = recall_score(y_true, y_pred, pos_label="match")

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)

"""#Example of Expected Match and Expected Non Match"""

filter_mask = match_file['source_image'] == "Niklas_Jozef_Baldis.jpg"
baldis_df = match_file[filter_mask]
baldis_df

filter_mask = match_file['source_image'] == "Alicia_Rand_Bodoia.jpg"
non_match = match_file[filter_mask]
non_match.head(1)

"""#Use Case 1: Face_Recognition"""

def compare_faces2(sourcefile, targetfile):
  similarity=0
  try:
    index1= '/content/content/SAM/2024_students/' + str(sourcefile)
    index2= '/content/content/SAM/2024_students/' + str(targetfile)
    encoding1= student_face_encodings[index1]
    encoding2= student_face_encodings[index2]
    distance= face_recognition.face_distance([encoding1],encoding2)[0]
    similarity= 1-distance
  except Exception as e:
    print(f'error occured {e}')
  return similarity

match_file2= pd.read_excel("/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/FinalMatchSourceTarget.xlsx")
match_file2.head()

#replace jpeg with jpg
match_file2['source_image'] = match_file2['source_image'].str.replace('.jpeg','.jpg')
match_file2['target_image'] = match_file2['target_image'].str.replace('.jpeg','.jpg')
match_file2 = match_file2[match_file2['source_image'] != '_aged.jpg']
match_file2 = match_file2[match_file2['target_image'] != '_aged.jpg']

match_file2['source_image'] = match_file2['source_image'].str.strip()
match_file2['target_image'] = match_file2['target_image'].str.strip()

match_file2.head()

match_file2['match_similarity']= match_file2.apply(lambda row: compare_faces2(row['source_image'], row['target_image']), axis=1)

match_file2.head()

import numpy as np
match_file2['predicted_match'] = np.where(match_file2['match_similarity'] > .5, 'match','no match')
match_file2.head()

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

# Calculate confusion matrix
matrix2 = confusion_matrix(match_file2['expected_match'],
                          match_file2['predicted_match'],
                          labels=["match", "no match"])

print(matrix2)

# Calculate confusion matrix
y_true = match_file2['expected_match']
y_pred = match_file2['predicted_match']
labels = ["match", "no match"]
matrix2 = confusion_matrix(y_true, y_pred, labels=labels)

# Plotting using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(matrix2, annot=True, fmt="d", xticklabels=labels, yticklabels=labels, cmap="Blues")
plt.title("Confusion Matrix")
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# Calculate Accuracy, Precision and Recall
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, pos_label="match")
recall = recall_score(y_true, y_pred, pos_label="match")

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)

"""#Example of Expected Match and Expected Non Match"""

filter_mask = match_file2['source_image'] == "Niklas_Jozef_Baldis.jpg"
baldis_df2 = match_file2[filter_mask]
baldis_df2

filter_mask = match_file2['source_image'] == "Alicia_Rand_Bodoia.jpg"
non_match2 = match_file2[filter_mask]
non_match2.head(1)

"""#Use Case 2: Face Recognition"""

#Download and unzip attendance
!gdown --id 1l4VhAN2vfPe4ySEuAWgyqyE92ndCJH6u
!unzip -o "attendance_2024.zip"

image1= 'IMG_1883.jpg'
target_image1= face_recognition.load_image_file(image1)
face_locations = face_recognition.face_locations(target_image1)
print('Number of faces found:', len(face_locations))

face_locations

# Define the URL of the image file on GitHub
image_file = "/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/Final Project Attendance (Use Case 2)/IMG_1883.jpg"

# open an image
image = Image.open(image_file)

# Get the original size of the image
width, height = image.size
print("Original image size:", width, height)

# Resize the image to half its original size
new_size = (int(width/4), int(height/4))
resized_image = image.resize(new_size)

new_width, new_height = resized_image.size
print("Resized image size:", new_width, new_height)

# Display the image in the Colab notebook
display(resized_image)

# Get the new size of the image

import cv2
# Convert the image from RGB to BGR
image = cv2.cvtColor(target_image1, cv2.COLOR_RGB2BGR)

# Loop over each face location and draw a rectangle
for (top, right, bottom, left) in face_locations:
    # Draw a rectangle around the face
    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)

# Display the image with rectangles around the faces
cv2_imshow(image)

student_face_encodings_list = list(student_face_encodings.values())
student_face_encodings_list2 = [element for element in student_face_encodings_list if not isinstance(element,int)]

# Get the locations and encodings of all faces in the image
face_locations = face_recognition.face_locations(target_image1)
face_encodings = face_recognition.face_encodings(target_image1, face_locations)

# Loop over each face encoding and compare it to the encodings in the dictionary
for encoding in face_encodings:
  if encoding.shape ==(128,):
    # Calculate the distance between the encoding and each encoding in the dictionary
    distances = face_recognition.face_distance(student_face_encodings_list2, encoding)

    # Find the index of the closest match in the source dictionary
    closest_match_index = distances.argmin()

    # Get the name of the closest match in the source dictionary
    closest_match_name = list(student_face_encodings.keys())[closest_match_index]

    # Print the name of the closest match
    print("Closest match:", closest_match_name)

from logging.config import fileConfig

def search_attendance(target_file):
  attendance_list = []
  # Load the image
  target_img = face_recognition.load_image_file(target_file)
  # convert image
  image = cv2.cvtColor(target_img, cv2.COLOR_RGB2BGR)

  # Get the locations of all faces in the image
  face_locations = face_recognition.face_locations(target_img)

  # Print the number of faces detected
  print("Number of faces detected:", len(face_locations))

  # Get the locations and encodings of all faces in the image
  face_locations = face_recognition.face_locations(target_img)
  face_encodings = face_recognition.face_encodings(target_image1, face_locations)
  student_face_encodings_list= list(student_face_encodings.values())

  # Loop over each face encoding in the target image and compare it to the encodings in the source dictionary
  for (top, right, bottom, left), encoding in zip(face_locations, face_encodings):
      # Calculate the distance between the encoding and each encoding in the dictionary
      distances = face_recognition.face_distance(student_face_encodings_list2, encoding)

      # Find the index of the closest match in the source dictionary
      closest_match_index = distances.argmin()

      # Get the name of the closest match in the source dictionary
      closest_match_name = list(student_face_encodings.keys())[closest_match_index]
      similarity = 1 -  distances[closest_match_index]
      # Get the file name without the extension
      file_name = os.path.splitext(os.path.basename(closest_match_name))[0]
      # Split the file name on the underscore character and join the first and last names with a space

      last_space_index = file_name.rfind(" ")
      if last_space_index != -1:
          first_name = file_name[:last_space_index]
          last_name = file_name[last_space_index+1:].rstrip("_1").rstrip("_2")
          full_name = "{} {}({:.0%})".format(first_name,last_name,similarity)
          #full_name = "{} {}".format(first_name,last_name)
      else:
          full_name = file_name

      # Add the period to the end of the full name
      full_name_period = f"{full_name}."

      # Draw a label on the image
      cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)
      cv2.putText(image, full_name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.7, (255, 255, 255), 1)

      # Print the name of the closest match
      print("Closest match:", full_name)
      attendance_list.append(full_name)


  cv2_imshow(image)
  return attendance_list, image

target_file = '/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/Final Project Attendance (Use Case 2)/IMG_1883.jpg'
attendence_list, output_image = search_attendance(target_file)

target_file = '/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/Final Project Attendance (Use Case 2)/IMG_1884.jpg'
attendence_list, output_image = search_attendance(target_file)

target_file = '/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/Final Project Attendance (Use Case 2)/IMG_1886.jpg'
attendence_list, output_image = search_attendance(target_file)

"""#Use Case 2: AWS"""

# AWS SDK for Python
import boto3
from botocore.exceptions import ClientError

# Image processing
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageOps, ImageEnhance

# Data analysis and visualization
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Standard Python libraries for web requests and file handling
import requests
import zipfile
import os
import re
import io

# Display utilities
from IPython.display import display

# create collection
client = boto3.client('rekognition')
collection_id = 'baldis_Case_2_collection'

try:
    client.create_collection(CollectionId=collection_id)
except ClientError as error:
    error_code = error.response['Error']['Code']
    if error_code == 'ResourceAlreadyExistsException':
        print('Collection already exists.')
    else:
        print(f'An error occurred: {error.response["Error"]["Message"]}')

import boto3
from botocore.exceptions import ClientError

client = boto3.client('rekognition')
#collection_id = 'your-collection-id'  # Replace with your collection ID

# Delete the collection
try:
    delete_response = client.delete_collection(CollectionId=collection_id)
    print(f"Collection '{collection_id}' deleted. Status Code: {delete_response['StatusCode']}")
except ClientError as error:
    print(f"Error occurred while deleting collection: {error.response['Error']['Message']}")

# Recreate the collection
try:
    create_response = client.create_collection(CollectionId=collection_id)
    print(f"Collection '{collection_id}' created. Status Code: {create_response['StatusCode']}")
except ClientError as error:
    print(f"Error occurred while creating collection: {error.response['Error']['Message']}")



#headshots_list
headshots_list = os.listdir('/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/Final Project Headshots&Aged (Use Case 1)')

#headshots_list.remove('ipynb_checkpoints')
headshots_list

# index all the headshots
s3_bucket = 'baldis-msba-headshots'

for image in headshots_list:
    # print(image)
    # make sure the names adhear to requirements
    externalImageId = (image
                      .replace(" ", "_")
                      .replace(",","_")
                      .replace("(","")
                      .replace(")","")
                      .replace(".jpg","")
                      .replace("_1","")
                      .replace("_2","")
                     )
    # make a name that you can use!
    externalImageId = re.sub(r'[^a-zA-Z0-9_:]', '', externalImageId)

    # index the faces
    client.index_faces(CollectionId=collection_id,
                                               Image={'S3Object': {'Bucket': s3_bucket, 'Name': image}},
                                               ExternalImageId=externalImageId, # this is the name of the person!!!
                                               MaxFaces=1,
                                               QualityFilter="AUTO",
                                               DetectionAttributes=['ALL'])
    print(f" indexed: {externalImageId}")

import boto3
import io
from PIL import Image, ImageDraw, ImageFont

def download_image_from_s3(bucket, image_key):
    s3_client = boto3.client('s3')
    s3_response = s3_client.get_object(Bucket=bucket, Key=image_key)
    image_content = s3_response['Body'].read()

    return Image.open(io.BytesIO(image_content))

def crop_image(bucket, image_key, box):
    img = download_image_from_s3(bucket, image_key)
    cropped_img = img.crop(box)
    in_mem_file = io.BytesIO()
    cropped_img.save(in_mem_file, format='JPEG')  # Specify format as needed
    in_mem_file.seek(0)
    return in_mem_file

def detect_faces(bucket, image_key):
    client = boto3.client('rekognition')
    response = client.detect_faces(
        Image={'S3Object': {'Bucket': bucket, 'Name': image_key}},
        Attributes=['DEFAULT']
    )
    return response['FaceDetails']

def search_face_by_image(collection_id, bucket, image_key, box):
    client = boto3.client('rekognition')
    # crop the image
    cropped_image = crop_image(bucket, image_key, box)
    try:
        response = client.search_faces_by_image(CollectionId=collection_id,
                                                Image={'Bytes': cropped_image.read()})
        MatchedFaces = response['FaceMatches']
    except:
        MatchedFaces = []
    return MatchedFaces


def draw_bounding_box_with_label(image, box, matches):
    draw = ImageDraw.Draw(image)
    font = ImageFont.load_default()

    if matches:  # Check if the list of matches is not empty
        for match in matches:
            external_id = match['Face']['ExternalImageId']
            similarity = match['Similarity']

            draw.rectangle(box, outline='lightgreen', width=3)
            label = f"ID: {external_id}Sim: {similarity:.2f}%"
            draw.text((box[0], box[3] + 10), label, fill="white", font=font)
    else:
        # Optional: Draw a box and label indicating no match
        draw.rectangle(box, outline='red', width=3)
        draw.text((box[0], box[3] + 10), "No Match", fill="white", font=font)

    return image

def draw_bounding_box_with_label(image, box, match):
    draw = ImageDraw.Draw(image)
    font = ImageFont.load_default()

    # Access 'ExternalImageId' and 'Similarity' correctly from the nested 'Face' dictionary
    external_id = match['Face'].get('ExternalImageId', 'Unknown')  # Use .get for safer access
    similarity = match.get('Similarity', 0)  # Default to 0 if key is missing

    draw.rectangle(box, outline='lightgreen', width=4)
    label = f"ID: {external_id} ({similarity:.2f}%)"
    draw.text((box[0], box[3] + 10), label, fill="white", font=font)  # Adjust text position if necessary

    return image

"""#Image 1"""

# Main Code for IMG_1883
bucket_name = 'baldis-attendance'
image_key = 'IMG_1883.jpg' # attendance image
collection_id = 'baldis_Case_2_collection'

# Download the image once
img = download_image_from_s3(bucket_name, image_key)
image_width, image_height = img.size

# detect all faces in attendance
faces = detect_faces(bucket_name, image_key)

# empty list to store matches
match_list = []
match_count = 0
nomatch_count = 0

# -- MAIN LOOP --
for face in faces:
    box = face['BoundingBox']
    box_coordinates = (
        int(box['Left'] * image_width),
        int(box['Top'] * image_height),
        int(box['Left'] * image_width + box['Width'] * image_width),
        int(box['Top'] * image_height + box['Height'] * image_height)
    )

    matches = search_face_by_image(collection_id, bucket_name, image_key, box_coordinates)
    #print("Matches for face:", matches)

    # Check if there are matches and draw bounding box
    if matches:
        for match in matches:
            img = draw_bounding_box_with_label(img, box_coordinates, match)
            match_list.append(match)
            match_count = match_count + 1
    else:
        # Optional: Draw a box indicating no match
        draw = ImageDraw.Draw(img)
        font = ImageFont.load_default()
        draw.rectangle(box_coordinates, outline='red', width=3)
        draw.text((box_coordinates[0], box_coordinates[3] + 10), "No Match", fill="white", font=font)
        nomatch_count = nomatch_count + 1
# Display or save the image as needed
img.show()
img.save(f'{image_key}_annotated_image.jpg')
print(f"** Matched {match_count} face count")
print(f"** No-match {nomatch_count} face count")

import pandas as pd

# Flatten the nested dictionaries
flattened_data = []
for entry in match_list:
    flattened_entry = {}
    flattened_entry.update(entry)
    flattened_entry.update(entry['Face'])
    flattened_entry.update(entry['Face']['BoundingBox'])
    del flattened_entry['Face']
    flattened_entry['BoundingBox'] = str(flattened_entry['BoundingBox'])  # If you want to keep the BoundingBox as a string representation
    flattened_data.append(flattened_entry)

# Create the DataFrame
df = pd.DataFrame(flattened_data)

df

# Set the style and color palette of the plot
sns.set(style="whitegrid", palette="pastel")

# Set the figure size
plt.figure(figsize=(12, 6))  # Width, Height in inches

# Create the histogram
sns.histplot(data=df, x='Similarity', bins=10, kde=True, color='skyblue', edgecolor='black')

# Add a title and labels
plt.title("Histogram of Similarity Scores", fontsize=18, fontweight='bold', color='navy')
plt.xlabel('Similarity Score', fontsize=14, fontweight='bold')
plt.ylabel('Frequency', fontsize=14, fontweight='bold')

# Customize the ticks
plt.xticks(fontsize=12, fontweight='bold', color='green')
plt.yticks(fontsize=12, fontweight='bold', color='green')

# Add a grid for easier reading
plt.grid(color='gray', linestyle='--', linewidth=0.5)

# Show the plot
plt.show()

# IF running in COLAB!!!
from IPython.display import display

# ... [rest of your code] ...

# Check if there are matches and draw bounding box
if matches:
    for match in matches:
        img = draw_bounding_box_with_label(img, box_coordinates, match)
        match_list.append(match)
        match_count = match_count + 1
else:
    # Optional: Draw a box indicating no match
    draw = ImageDraw.Draw(img)
    font = ImageFont.load_default()
    draw.rectangle(box_coordinates, outline='red', width=3)
    draw.text((box_coordinates[0], box_coordinates[3] + 10), "No Match", fill="white", font=font)
    nomatch_count = nomatch_count + 1

# Display the image in Colab
display(img)
# this may need to be cv_showimage(img)

# Optionally save the image
img.save(f'{image_key}_annotated_image.jpg')

# Print face match counts
print(f"** Matched {match_count} face count")
print(f"** No-match {nomatch_count} face count")

"""#Image 2"""

# Main Code for IMG_1884
bucket_name = 'baldis-attendance'
image_key = 'IMG_1884.jpg' # attendance image
collection_id = 'baldis_Case_2_collection'

# Download the image once
img = download_image_from_s3(bucket_name, image_key)
image_width, image_height = img.size

# detect all faces in attendance
faces = detect_faces(bucket_name, image_key)

# empty list to store matches
match_list = []
match_count = 0
nomatch_count = 0

# -- MAIN LOOP --
for face in faces:
    box = face['BoundingBox']
    box_coordinates = (
        int(box['Left'] * image_width),
        int(box['Top'] * image_height),
        int(box['Left'] * image_width + box['Width'] * image_width),
        int(box['Top'] * image_height + box['Height'] * image_height)
    )

    matches = search_face_by_image(collection_id, bucket_name, image_key, box_coordinates)
    #print("Matches for face:", matches)

    # Check if there are matches and draw bounding box
    if matches:
        for match in matches:
            img = draw_bounding_box_with_label(img, box_coordinates, match)
            match_list.append(match)
            match_count = match_count + 1
    else:
        # Optional: Draw a box indicating no match
        draw = ImageDraw.Draw(img)
        font = ImageFont.load_default()
        draw.rectangle(box_coordinates, outline='red', width=3)
        draw.text((box_coordinates[0], box_coordinates[3] + 10), "No Match", fill="white", font=font)
        nomatch_count = nomatch_count + 1
# Display or save the image as needed
img.show()
img.save(f'{image_key}_annotated_image.jpg')
print(f"** Matched {match_count} face count")
print(f"** No-match {nomatch_count} face count")

import pandas as pd

# Flatten the nested dictionaries
flattened_data = []
for entry in match_list:
    flattened_entry = {}
    flattened_entry.update(entry)
    flattened_entry.update(entry['Face'])
    flattened_entry.update(entry['Face']['BoundingBox'])
    del flattened_entry['Face']
    flattened_entry['BoundingBox'] = str(flattened_entry['BoundingBox'])  # If you want to keep the BoundingBox as a string representation
    flattened_data.append(flattened_entry)

# Create the DataFrame
df = pd.DataFrame(flattened_data)

df

# Set the style and color palette of the plot
sns.set(style="whitegrid", palette="pastel")

# Set the figure size
plt.figure(figsize=(12, 6))  # Width, Height in inches

# Create the histogram
sns.histplot(data=df, x='Similarity', bins=10, kde=True, color='skyblue', edgecolor='black')

# Add a title and labels
plt.title("Histogram of Similarity Scores", fontsize=18, fontweight='bold', color='navy')
plt.xlabel('Similarity Score', fontsize=14, fontweight='bold')
plt.ylabel('Frequency', fontsize=14, fontweight='bold')

# Customize the ticks
plt.xticks(fontsize=12, fontweight='bold', color='green')
plt.yticks(fontsize=12, fontweight='bold', color='green')

# Add a grid for easier reading
plt.grid(color='gray', linestyle='--', linewidth=0.5)

# Show the plot
plt.show()

# IF running in COLAB!!!
from IPython.display import display

# ... [rest of your code] ...

# Check if there are matches and draw bounding box
if matches:
    for match in matches:
        img = draw_bounding_box_with_label(img, box_coordinates, match)
        match_list.append(match)
        match_count = match_count + 1
else:
    # Optional: Draw a box indicating no match
    draw = ImageDraw.Draw(img)
    font = ImageFont.load_default()
    draw.rectangle(box_coordinates, outline='red', width=3)
    draw.text((box_coordinates[0], box_coordinates[3] + 10), "No Match", fill="white", font=font)
    nomatch_count = nomatch_count + 1

# Display the image in Colab
display(img)
# this may need to be cv_showimage(img)

# Optionally save the image
img.save(f'{image_key}_annotated_image.jpg')

# Print face match counts
print(f"** Matched {match_count} face count")
print(f"** No-match {nomatch_count} face count")

"""#Image 3"""

# Main Code for IMG_1886
bucket_name = 'baldis-attendance'
image_key = 'IMG_1886.jpg' # attendance image
collection_id = 'baldis_Case_2_collection'

# Download the image once
img = download_image_from_s3(bucket_name, image_key)
image_width, image_height = img.size

# detect all faces in attendance
faces = detect_faces(bucket_name, image_key)

# empty list to store matches
match_list = []
match_count = 0
nomatch_count = 0

# -- MAIN LOOP --
for face in faces:
    box = face['BoundingBox']
    box_coordinates = (
        int(box['Left'] * image_width),
        int(box['Top'] * image_height),
        int(box['Left'] * image_width + box['Width'] * image_width),
        int(box['Top'] * image_height + box['Height'] * image_height)
    )

    matches = search_face_by_image(collection_id, bucket_name, image_key, box_coordinates)
    #print("Matches for face:", matches)

    # Check if there are matches and draw bounding box
    if matches:
        for match in matches:
            img = draw_bounding_box_with_label(img, box_coordinates, match)
            match_list.append(match)
            match_count = match_count + 1
    else:
        # Optional: Draw a box indicating no match
        draw = ImageDraw.Draw(img)
        font = ImageFont.load_default()
        draw.rectangle(box_coordinates, outline='red', width=3)
        draw.text((box_coordinates[0], box_coordinates[3] + 10), "No Match", fill="white", font=font)
        nomatch_count = nomatch_count + 1
# Display or save the image as needed
img.show()
img.save(f'{image_key}_annotated_image.jpg')
print(f"** Matched {match_count} face count")
print(f"** No-match {nomatch_count} face count")

import pandas as pd

# Flatten the nested dictionaries
flattened_data = []
for entry in match_list:
    flattened_entry = {}
    flattened_entry.update(entry)
    flattened_entry.update(entry['Face'])
    flattened_entry.update(entry['Face']['BoundingBox'])
    del flattened_entry['Face']
    flattened_entry['BoundingBox'] = str(flattened_entry['BoundingBox'])  # If you want to keep the BoundingBox as a string representation
    flattened_data.append(flattened_entry)

# Create the DataFrame
df = pd.DataFrame(flattened_data)

df

# Set the style and color palette of the plot
sns.set(style="whitegrid", palette="pastel")

# Set the figure size
plt.figure(figsize=(12, 6))  # Width, Height in inches

# Create the histogram
sns.histplot(data=df, x='Similarity', bins=10, kde=True, color='skyblue', edgecolor='black')

# Add a title and labels
plt.title("Histogram of Similarity Scores", fontsize=18, fontweight='bold', color='navy')
plt.xlabel('Similarity Score', fontsize=14, fontweight='bold')
plt.ylabel('Frequency', fontsize=14, fontweight='bold')

# Customize the ticks
plt.xticks(fontsize=12, fontweight='bold', color='green')
plt.yticks(fontsize=12, fontweight='bold', color='green')

# Add a grid for easier reading
plt.grid(color='gray', linestyle='--', linewidth=0.5)

# Show the plot
plt.show()

# IF running in COLAB!!!
from IPython.display import display

# ... [rest of your code] ...

# Check if there are matches and draw bounding box
if matches:
    for match in matches:
        img = draw_bounding_box_with_label(img, box_coordinates, match)
        match_list.append(match)
        match_count = match_count + 1
else:
    # Optional: Draw a box indicating no match
    draw = ImageDraw.Draw(img)
    font = ImageFont.load_default()
    draw.rectangle(box_coordinates, outline='red', width=3)
    draw.text((box_coordinates[0], box_coordinates[3] + 10), "No Match", fill="white", font=font)
    nomatch_count = nomatch_count + 1

# Display the image in Colab
display(img)
# this may need to be cv_showimage(img)

# Optionally save the image
img.save(f'{image_key}_annotated_image.jpg')

# Print face match counts
print(f"** Matched {match_count} face count")
print(f"** No-match {nomatch_count} face count")

#Convert to HTML
#%%shell
#jupyter nbconvert --to html "/content/drive/MyDrive/Colab Notebooks/Data Management- FS/Final Project/Baldis_Nik_Final_Project.ipynb"
